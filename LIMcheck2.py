import pandas as pd
import numpy as np
import os
import glob
import subprocess
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, confusion_matrix

# ==========================================
# 1. ä½ çš„ Top 10 é»„é‡‘ç‰¹å¾ (å¿…é¡»ä¸¥æ ¼ä¸€è‡´)
# ==========================================
MY_TOP_10 = [
    "chr7:27184167", "chr8:22859168", "chr2:216585197", 
    "chr2:216585387", "chr2:216584917", "chr3:138069944", 
    "chr2:216585497", "chr7:27183697", "chr1:56957682", 
    "chr2:216584427"
]

# ==========================================
# 2. è·¯å¾„é…ç½® (è¯·å†æ¬¡ç¡®è®¤)
# ==========================================
# å­˜æ”¾ BAM æ–‡ä»¶çš„æ–‡ä»¶å¤¹
BAM_DIR = "External_Validation/results" 
# å­˜æ”¾å…ƒæ•°æ®çš„ CSV
META_FILE = "External_Validation/SraRunTable.csv" 
# ä½ çš„ TCGA è®­ç»ƒæ•°æ®
TRAIN_FILE = "TCGA_RPKM_eRNA_300k_peaks_in_Super_enhancer_BRCA.txt"

print(f"{'='*50}")
print("ğŸš€ æ­£åœ¨å¯åŠ¨ï¼šæ ¸å¼¹çº§å¤–éƒ¨éªŒè¯ (Re-Quantification Pipeline)")
print(f"{'='*50}")

# ==========================================
# Step 1: åˆ¶ä½œ SAF åˆ»åº¦å°º
# ==========================================
print("\n>>> [1/5] ç”Ÿæˆ SAF æ³¨é‡Šæ–‡ä»¶...")
saf_rows = []
for feat in MY_TOP_10:
    # æ‹†åˆ†åæ ‡
    chrom, pos = feat.split(':')
    # è®¾å®šå®½åº¦ï¼šé˜Ÿå‹è™½ç„¶è¿‡æ»¤äº†ï¼Œä½†æˆ‘ä»¬é‡æ–°å®šé‡è¦ç»™å¤Ÿå®½åº¦
    # å‡è®¾ peak å®½åº¦ä¸º 500bp (+/- 250)
    start = int(pos) - 250
    end = int(pos) + 250
    saf_rows.append([feat, chrom, start, end, '+'])

df_saf = pd.DataFrame(saf_rows, columns=['GeneID', 'Chr', 'Start', 'End', 'Strand'])
df_saf.to_csv("my_top10.saf", sep='\t', index=False, header=False)
print(" âœ… my_top10.saf å·²ç”Ÿæˆ")

# ==========================================
# Step 2: è¿è¡Œ featureCounts (æ•°æ•°)
# ==========================================
print("\n>>> [2/5] æ­£åœ¨è¿è¡Œ featureCounts (ç›´æ¥è¯»å– BAM)...")
bam_files = glob.glob(os.path.join(BAM_DIR, "*.bam"))

if not bam_files:
    print(f"âŒ è‡´å‘½é”™è¯¯: åœ¨ {BAM_DIR} ä¸‹æ²¡æ‰¾åˆ°ä»»ä½• .bam æ–‡ä»¶ï¼æ— æ³•éªŒè¯ã€‚")
    exit()

print(f"   å‘ç° {len(bam_files)} ä¸ª BAM æ–‡ä»¶ï¼Œå¼€å§‹å®šé‡...")

# æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç»“æœï¼Œé¿å…é‡å¤è·‘ (featureCounts æŒºå¿«çš„ï¼Œä½†èƒ½çœåˆ™çœ)
output_counts = "my_direct_counts.txt"
if os.path.exists(output_counts) and os.path.getsize(output_counts) > 100:
    print("   âš ï¸ æ£€æµ‹åˆ°å·²å­˜åœ¨è®¡æ•°æ–‡ä»¶ï¼Œè·³è¿‡å®šé‡æ­¥éª¤ï¼Œç›´æ¥ä½¿ç”¨ç°æˆæ–‡ä»¶ã€‚")
    print("   (å¦‚æœæƒ³é‡æ–°è·‘ï¼Œè¯·æ‰‹åŠ¨åˆ é™¤ my_direct_counts.txt)")
else:
    cmd = [
        "featureCounts",
        "-T", "8",                  # çº¿ç¨‹
        "-p",                       # åŒç«¯æµ‹åº
        "-F", "SAF",                # æ ¼å¼
        "-a", "my_top10.saf",       # æ³¨é‡Š
        "-o", output_counts,        # è¾“å‡º
        "-M",                       # ã€å…³é”®ã€‘ç»Ÿè®¡ Multi-mapping reads (æ•‘å›é‚£å‡ åƒä¸‡æ¡æ•°æ®)
        "-O",                       # Allow Multi-overlap
        "--fraction",               # å¦‚æœä¸€æ¡readæ¯”å¯¹åˆ°3ä¸ªåœ°æ–¹ï¼Œæ¯ä¸ªåœ°æ–¹ç®—1/3 (æ›´ç§‘å­¦)
        "-s", "0"                   # ã€å…³é”®ã€‘å¿½ç•¥é“¾çš„æ–¹å‘ (Unstranded)ï¼Œé˜²æ­¢æ­£è´Ÿé“¾æå
    ] + bam_files
    
    try:
        subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        print(" âœ… å®šé‡å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ featureCounts è¿è¡Œå¤±è´¥: {e}")
        print("   è¯·ç¡®è®¤æœåŠ¡å™¨å®‰è£…äº† subread (conda install subread)")
        exit()

# ==========================================
# Step 3: æ•°æ®æ¸…æ´—ä¸æ ‡å‡†åŒ–
# ==========================================
print("\n>>> [3/5] å¤„ç†éªŒè¯é›†æ•°æ®...")
val_df = pd.read_csv(output_counts, sep='\t', comment='#', index_col=0)
# æå–æ•°æ®åˆ— (ç¬¬6åˆ—ä»¥å)
val_X = val_df.iloc[:, 5:].T

# æ¸…æ´—æ ·æœ¬å: '.../SRR12345.bam' -> 'SRR12345'
val_X.index = [os.path.basename(x).split('.')[0] for x in val_X.index]
print(f"   æå–åˆ° {val_X.shape[0]} ä¸ªå¤–éƒ¨æ ·æœ¬ã€‚")

# --- ç®€å•çš„ CPM å½’ä¸€åŒ– ---
# å› ä¸º TCGA æ˜¯ RPKMï¼Œè¿™é‡Œç”¨ CPM (Counts Per Million) è¿‘ä¼¼æ›¿ä»£
# log2(CPM + 1)
lib_sizes = val_X.sum(axis=1)
val_X_cpm = val_X.div(lib_sizes, axis=0) * 1e6
val_X_final = np.log2(val_X_cpm + 1)

# ==========================================
# Step 4: é‡æ–°è®­ç»ƒ TCGA æ¨¡å‹
# ==========================================
print("\n>>> [4/5] ç”¨ Top 10 ç‰¹å¾é‡è®­ TCGA æ¨¡å‹...")
try:
    tcga = pd.read_csv(TRAIN_FILE, sep='\t', index_col=0)
    # åªå–è¿™10ä¸ª
    X_train = tcga.loc[MY_TOP_10].T
except KeyError as e:
    print(f"âŒ è®­ç»ƒå¤±è´¥: TCGA åŸå§‹æ•°æ®é‡Œæ‰¾ä¸åˆ°è¿™äº›ç‰¹å¾: {e}")
    exit()

# åˆ¶ä½œæ ‡ç­¾
y_train = []
for idx in X_train.index:
    if '01' in idx[13:15] or '_tumor' in idx:
        y_train.append(1)
    elif '11' in idx[13:15] or '_normal' in idx:
        y_train.append(0)
    else:
        y_train.append(-1) # ä¸¢å¼ƒ

y_train = np.array(y_train)
mask = y_train != -1
X_train = X_train[mask]
y_train = y_train[mask]

# Logè½¬æ¢ (TCGAæ•°æ®å¦‚æœæœ¬èº«æ²¡logï¼Œè¿™é‡Œè¦logï¼›å¦‚æœä½ çš„æ–‡ä»¶å·²ç»æ˜¯RPKMï¼Œé€šå¸¸éœ€è¦log)
# å‡è®¾ä½ çš„æ–‡ä»¶æ˜¯ RPKM/FPKM raw value
if X_train.max().max() > 100:
    X_train = np.log2(X_train + 1)

# æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train_scaled, y_train)
print(" âœ… æ¨¡å‹è®­ç»ƒå®Œæ¯•ã€‚")

# ==========================================
# Step 5: é¢„æµ‹ä¸ç»“æœåŒ¹é…
# ==========================================
print("\n>>> [5/5] æœ€ç»ˆé¢„æµ‹ä¸éªŒè¯...")

# ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
val_X_final = val_X_final[MY_TOP_10]
val_X_scaled = scaler.transform(val_X_final) # ç”¨è®­ç»ƒé›†çš„scaler

# é¢„æµ‹æ¦‚ç‡
probs = clf.predict_proba(val_X_scaled)[:, 1]
val_X_final['Pred_Prob'] = probs

# å°è¯•è¯»å–å…ƒæ•°æ®è®¡ç®— AUC
try:
    meta = pd.read_csv(META_FILE)
    # è‡ªåŠ¨å¯»æ‰¾ ID åˆ— (SRR...)
    id_col = None
    for col in meta.columns:
        if meta[col].astype(str).str.contains('SRR').any():
            id_col = col
            break
    
    # è‡ªåŠ¨å¯»æ‰¾ Label åˆ—
    label_col = None
    for col in meta.columns:
        if meta[col].astype(str).str.lower().isin(['tumor', 'cancer', 'normal', 'tissue']).any():
            label_col = col
            # ä¼˜å…ˆæ‰¾ explicit çš„
            if 'source_name' in col or 'Group' in col:
                break
    
    if id_col and label_col:
        print(f"   è‡ªåŠ¨è¯†åˆ«å…ƒæ•°æ®: IDåˆ—=[{id_col}], åˆ†ç»„åˆ—=[{label_col}]")
        
        # æ˜ å°„å­—å…¸
        meta_dict = dict(zip(meta[id_col], meta[label_col]))
        
        y_true = []
        y_scores = []
        
        print("\n   --- è¯¦ç»†é¢„æµ‹ç»“æœ ---")
        print(f"   {'SampleID':<15} {'True_Label':<20} {'Pred_Prob (Cancer)':<10}")
        print("-" * 50)
        
        for sid in val_X_final.index:
            if sid in meta_dict:
                true_label_str = str(meta_dict[sid])
                prob = val_X_final.loc[sid, 'Pred_Prob']
                
                # ç®€å•çš„å…³é”®è¯åˆ¤æ–­
                is_cancer = 1 if ('tumor' in true_label_str.lower() or 'cancer' in true_label_str.lower()) else 0
                if 'normal' in true_label_str.lower(): is_cancer = 0
                
                y_true.append(is_cancer)
                y_scores.append(prob)
                
                print(f"   {sid:<15} {true_label_str[:20]:<20} {prob:.4f}")
        
        if len(y_true) > 0:
            auc = roc_auc_score(y_true, y_scores)
            print(f"\n{'='*30}")
            print(f"ğŸ† å¤–éƒ¨éªŒè¯ AUC: {auc:.4f}")
            print(f"{'='*30}")
            if auc > 0.8:
                print("ğŸ‰ æ­å–œï¼ç»“æœéå¸¸æ£’ï¼æ–‡ç« ç¨³äº†ï¼")
            elif auc > 0.6:
                print("ğŸ†— ç»“æœè¿˜å¯ä»¥ï¼Œæœ‰é¢„æµ‹æ½œåŠ›ã€‚")
            else:
                print("âš ï¸ ç»“æœä¸€èˆ¬ï¼Œå¯èƒ½å­˜åœ¨æ‰¹æ¬¡æ•ˆåº”ï¼Œæˆ–Top10åœ¨å¤–éƒ¨æ•°æ®ä¸è¡¨è¾¾ã€‚")
    else:
        print("âš ï¸ æ— æ³•è‡ªåŠ¨è§£æå…ƒæ•°æ®åˆ—åï¼Œè¯·æ‰‹åŠ¨æŸ¥çœ‹ my_direct_counts.txt å’Œé¢„æµ‹æ¦‚ç‡ã€‚")

except Exception as e:
    print(f"âš ï¸ å…ƒæ•°æ®å¤„ç†å‡ºé”™: {e}")
    print("   é¢„æµ‹æ¦‚ç‡å·²ä¿å­˜åœ¨ val_X_final DataFrame ä¸­ã€‚")