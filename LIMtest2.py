# ç™Œæ— + LASSO

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectFromModel, VarianceThreshold
from sklearn.metrics import roc_auc_score, confusion_matrix
from sklearn.preprocessing import StandardScaler

FILE_PATH = 'TCGA_RPKM_eRNA_300k_peaks_in_Super_enhancer_BRCA.txt' 

# éšæœºç§å­ (ä¿è¯æ¯æ¬¡æŠ½æ ·ç»“æœä¸€è‡´)
SEED = 42

print(f">>> æ­£åœ¨è¯»å–æ–‡ä»¶: {FILE_PATH} ...")
try:
    # å‡è®¾æ–‡ä»¶æ˜¯åˆ¶è¡¨ç¬¦åˆ†éš”ï¼Œè¡Œæ˜¯eRNAï¼Œåˆ—æ˜¯æ ·æœ¬
    df = pd.read_csv(FILE_PATH, sep='\t', index_col=0)
    print(f"   åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
except Exception as e:
    print(f"âŒ è¯»å–å¤±è´¥: {e}")
    exit()

# ==========================================
# 2. æ ·æœ¬è¯†åˆ«ä¸æå– (åªç”¨ TCGA å†…éƒ¨)
# ==========================================
cols = df.columns.tolist()

# å°è¯•è¯†åˆ« 01 (Tumor) å’Œ 11 (Normal)
# é€»è¾‘ï¼šå¦‚æœåˆ—åç¬¦åˆ TCGA æ ‡å‡† (ç¬¬14-15ä½)ï¼Œæˆ–è€…æ˜¯ _tumor/_normal åç¼€
tumor_cols = []
normal_cols = []

if any('_tumor' in c for c in cols):
    tumor_cols = [c for c in cols if '_tumor' in c]
    normal_cols = [c for c in cols if '_normal' in c]
else:
    # å‡è®¾æ˜¯æ ‡å‡† Barcodeï¼Œåˆ‡ç‰‡æ£€æŸ¥
    # å¦‚æœåˆ—åä¸å¤Ÿé•¿ï¼Œå¯èƒ½ä¼šæŠ¥é”™ï¼Œè¿™é‡ŒåŠ ä¸ªåˆ¤æ–­
    tumor_cols = [c for c in cols if len(c) > 15 and c[13:15] == '01']
    normal_cols = [c for c in cols if len(c) > 15 and c[13:15] == '11']

print(f"   è¯†åˆ«åˆ° Tumor (ç™Œç—‡): {len(tumor_cols)}")
print(f"   è¯†åˆ«åˆ° Normal (ç™Œæ—): {len(normal_cols)}")

if len(normal_cols) < 10:
    print("âŒ æ­£å¸¸æ ·æœ¬å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œå®éªŒï¼è¯·æ£€æŸ¥æ–‡ä»¶åˆ—åæ ¼å¼ã€‚")
    exit()

# ==========================================
# 3. ä¸‹é‡‡æ · (Downsampling) - ä½ çš„æ ¸å¿ƒå®éªŒé€»è¾‘
# ==========================================
print(f"\n>>> æ­£åœ¨è¿›è¡Œã€ç­‰é‡æŠ½å–ã€‘å®éªŒ...")
print(f"   ç›®æ ‡ï¼šéšæœºæŠ½å– {len(normal_cols)} ä¸ªç™Œç—‡æ ·æœ¬ï¼Œä¸ç™Œæ— 1:1 é…å¯¹")

# éšæœºæŠ½å–ä¸ normal æ•°é‡ä¸€è‡´çš„ tumor
np.random.seed(SEED)
selected_tumor_cols = np.random.choice(tumor_cols, size=len(normal_cols), replace=False)

# æ„å»ºæœ€ç»ˆæ•°æ®é›†
df_tumor = df[selected_tumor_cols].T
df_normal = df[normal_cols].T

df_tumor['Label'] = 1  # ç™Œç—‡ä¸º 1
df_normal['Label'] = 0 # ç™Œæ—ä¸º 0

full_data = pd.concat([df_tumor, df_normal])

# æ£€æŸ¥ eRNA ç‰¹å¾æ˜¯å¦éœ€è¦ Log è½¬æ¢ (å¦‚æœæœ€å¤§å€¼ > 100 å°±è½¬)
X = full_data.drop(columns=['Label'])
y = full_data['Label']

if X.max().max() > 100:
    print("   æ•°å€¼è¾ƒå¤§ï¼Œåº”ç”¨ Log2(x+1) è½¬æ¢...")
    X = np.log2(X + 1)

# ==========================================
# 4. åˆ’åˆ†æ•°æ®é›† (6:2:2)
# ==========================================
print("\n>>> æ­£åœ¨åˆ’åˆ†æ•°æ®é›† (60% è®­ç»ƒ, 20% éªŒè¯, 20% æµ‹è¯•)...")

# ç¬¬ä¸€åˆ€ï¼šåˆ‡å‡º 20% æµ‹è¯•é›† (Test)
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=SEED, stratify=y
)

# ç¬¬äºŒåˆ€ï¼šå‰©ä¸‹çš„ 80% é‡Œå†åˆ‡å‡º 25% ä½œä¸ºéªŒè¯é›† (0.8 * 0.25 = 0.2)
# è¿™æ ·æœ€ç»ˆæ¯”ä¾‹å°±æ˜¯ 6:2:2
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=SEED, stratify=y_temp
)

print(f"   è®­ç»ƒé›†: {X_train.shape[0]} (Tumor: {sum(y_train==1)}, Normal: {sum(y_train==0)})")
print(f"   éªŒè¯é›†: {X_val.shape[0]}")
print(f"   æµ‹è¯•é›†: {X_test.shape[0]}")

print(f"\n{'='*40}")
print(f"ğŸš€ å¼€å§‹ç‰¹å¾ç­›é€‰ (Feature Selection)")
print(f"{'='*40}")

# ==========================================
# 1. é¢„å¤„ç†ï¼šæ ‡å‡†åŒ– (å¯¹ LASSO è‡³å…³é‡è¦)
# ==========================================
scaler = StandardScaler()
X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)
X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)

print(f"åŸå§‹ç‰¹å¾æ•°: {X_train.shape[1]}")

# ==========================================
# 2. åˆç­›ï¼šæ–¹å·®è¿‡æ»¤ (Variance Threshold)
# ==========================================
# å‰”é™¤é‚£äº›åœ¨ 99% çš„æ ·æœ¬ä¸­æ•°å€¼éƒ½ä¸€æ ·çš„ç‰¹å¾ (å‡ ä¹æ²¡æœ‰åŒºåˆ†åº¦çš„)
# é˜ˆå€¼è®¾ä¸º 0.01 (æˆ–è€…æ›´ä¸¥æ ¼)
selector_var = VarianceThreshold(threshold=0.01)
X_train_var = selector_var.fit_transform(X_train_scaled)
X_test_var = selector_var.transform(X_test_scaled)

# è·å–å‰©ä½™ç‰¹å¾çš„åˆ—å
kept_indices = selector_var.get_support(indices=True)
kept_columns = X_train.columns[kept_indices]

# æ›´æ–° DataFrame
X_train_filtered = pd.DataFrame(X_train_var, columns=kept_columns, index=X_train.index)
X_test_filtered = pd.DataFrame(X_test_var, columns=kept_columns, index=X_test.index)

print(f"æ–¹å·®è¿‡æ»¤åå‰©ä½™ç‰¹å¾æ•°: {X_train_filtered.shape[1]}")

# ==========================================
# 3. æ ¸å¿ƒç­›é€‰ï¼šLASSO (Logistic Regression L1)
# ==========================================
print("\n>>> æ­£åœ¨è¿è¡Œ LASSO è¿›è¡Œç¨€ç–ç‰¹å¾é€‰æ‹©...")
# C å€¼è¶Šå°ï¼Œæ­£åˆ™åŒ–è¶Šå¼ºï¼Œé€‰å‡ºæ¥çš„ç‰¹å¾è¶Šå°‘ï¼›C å€¼è¶Šå¤§ï¼Œç‰¹å¾è¶Šå¤šã€‚
# å»ºè®®å°è¯• C=0.01, 0.05, 0.1, 0.5 æ¥æ§åˆ¶æ•°é‡
lasso = LogisticRegression(penalty='l1', C=0.1, solver='liblinear', random_state=42)
lasso.fit(X_train_filtered, y_train)

# è·å–ç³»æ•°ä¸ä¸º 0 çš„ç‰¹å¾
model_coef = lasso.coef_.flatten()
selected_mask = model_coef != 0
selected_features_lasso = X_train_filtered.columns[selected_mask].tolist()
selected_coefs = model_coef[selected_mask]

print(f"âœ… LASSO ç­›é€‰å‡º {len(selected_features_lasso)} ä¸ªé‡è¦ç‰¹å¾")

# å¦‚æœ LASSO é€‰å¤ªå¤šï¼Œæˆ‘ä»¬å¼ºåˆ¶å–ç»å¯¹å€¼ç³»æ•°æœ€å¤§çš„ Top 10
if len(selected_features_lasso) > 10:
    print("   (ç‰¹å¾ä¾ç„¶å¾ˆå¤šï¼Œå¼ºåˆ¶é€‰å–ç³»æ•°ç»å¯¹å€¼æœ€å¤§çš„ Top 10)")
    # åˆ›å»º (ç‰¹å¾å, ç³»æ•°ç»å¯¹å€¼) çš„åˆ—è¡¨å¹¶æ’åº
    feature_importance = list(zip(selected_features_lasso, np.abs(selected_coefs)))
    feature_importance.sort(key=lambda x: x[1], reverse=True)
    final_features = [x[0] for x in feature_importance[:10]]
else:
    final_features = selected_features_lasso

print(f"\nğŸ’ æœ€ç»ˆå…¥é€‰çš„ 'é»„é‡‘ eRNA' ({len(final_features)}ä¸ª):")
for i, f in enumerate(final_features):
    print(f"   {i+1}. {f}")

# ==========================================
# 4. ç»ˆæéªŒè¯ï¼šç”¨è¿™å‡ ä¸ªç‰¹å¾é‡è·‘æ¨¡å‹
# ==========================================
print(f"\n>>> æ­£åœ¨ä½¿ç”¨ Top {len(final_features)} ç‰¹å¾é‡æ„éšæœºæ£®æ—...")

# åªå–è¿™äº›åˆ—
X_train_final = X_train_scaled[final_features]
X_test_final = X_test_scaled[final_features]

# è®­ç»ƒå°æ¨¡å‹
clf_mini = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
clf_mini.fit(X_train_final, y_train)

# é¢„æµ‹
y_pred_prob = clf_mini.predict_proba(X_test_final)[:, 1]
auc_mini = roc_auc_score(y_test, y_pred_prob)

print(f"\n{'-'*40}")
print(f"ğŸ† Top {len(final_features)} ç‰¹å¾æ¨¡å‹æµ‹è¯•é›† AUC: {auc_mini:.4f}")
print(f"{'-'*40}")

# æ··æ·†çŸ©é˜µçœ‹ä¸€çœ¼
y_pred = clf_mini.predict(X_test_final)
print("æ··æ·†çŸ©é˜µ:")
print(confusion_matrix(y_test, y_pred))

# å¦‚æœéœ€è¦ä¿å­˜ç‰¹å¾åˆ—è¡¨ç”¨äºç”»å›¾
# pd.Series(final_features).to_csv("Diagnostic_Signature_Genes.csv", index=False)